{
 "metadata": {
  "name": "",
  "signature": "sha256:85b6bdfd95b0f251c03d855a7ef77d834daf093c7c6ff6b7320b0f604387f7ac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import tables\n",
      "import re\n",
      "import glob\n",
      "import os\n",
      "from StringIO import StringIO\n",
      "\n",
      "############\n",
      "## Aggregates data from all json files into cvs summary\n",
      "############\n",
      "\n",
      "data_path = \"/home/matt/Development/cs580/project/data/\"\n",
      "SALAMI_path = data_path + \"SALAMI/\" # json\n",
      "\n",
      "salami_ex = SALAMI_path + 'echonest_features/1060/echonest_data.json'\n",
      "\n",
      "# series -> index | split | records\n",
      "# frame  -> columns | split | records | index | values\n",
      "\n",
      "def gen_dataframe():\n",
      "    print \"ID, num_bars,avg_bar_len,std_bar_len,avg_bar_conf,std_bar_conf,\" \\\n",
      "              \"num_beats,avg_beat_len,std_beat_len,avg_beat_conf,std_beat_conf,\" \\\n",
      "              \"num_tatums,avg_tatum_len,std_tatum_len,avg_tatum_conf,std_tatum_conf,\" \\\n",
      "              \"num_sections,avg_section_len,std_section_len,avg_section_conf,std_section_conf,\"\\\n",
      "              \"duration,key_val,key_conf,tempo_val,tempo_conf,genre\"\n",
      "    for songpath in glob.glob(SALAMI_path + 'echonest_features/*/*.json'):\n",
      "        printRow(get_SALAMI_song_info(songpath))\n",
      "\n",
      "\n",
      "def get_SALAMI_song_info(songpath):\n",
      "    df = pd.read_json(songpath, typ='series')\n",
      "    metadata = df['metadata']\n",
      "    id = metadata['identifier']\n",
      "    bars_tuple     = collect_stats(df['bars'])\n",
      "    beats_tuple    = collect_stats(df['beats'])\n",
      "    tatums_tuple   = collect_stats(df['tatums'])\n",
      "    sections_tuple = collect_stats(df['sections'])\n",
      "    duration = metadata['duration']\n",
      "    key_val = metadata['key']['value']\n",
      "    key_conf = metadata['key']['confidence']\n",
      "    tempo_val = metadata['tempo']['value']\n",
      "    tempo_conf = metadata['tempo']['confidence']\n",
      "    if 'genre' in metadata.keys() and metadata['genre'] != \"\":\n",
      "        genre = metadata['genre']\n",
      "    else:\n",
      "        return None\n",
      "    clustering = getClusterData(songpath)\n",
      "    if clustering is None:\n",
      "        return None\n",
      "    ret = [id]\n",
      "    ret.extend(bars_tuple)\n",
      "    ret.extend(beats_tuple)\n",
      "    ret.extend(tatums_tuple)\n",
      "    ret.extend(sections_tuple)\n",
      "    ret.append(duration)\n",
      "    ret.append(key_val)\n",
      "    ret.append(key_conf)\n",
      "    ret.append(tempo_val)\n",
      "    ret.append(tempo_conf)\n",
      "    ret.extend(clustering)\n",
      "    ret.append(genre)\n",
      "    return tuple(ret)\n",
      "\n",
      "def getClusterData(songpath):\n",
      "    file_dir = re.match('[0-9]+',songpath)\n",
      "    filepath = SALAMI_path + 'data/' + file_dir + '/parsed/'\n",
      "    if os.path.isfile(filepath):\n",
      "        df = pd.read_table(filepath, delim_whitespace=true, names=['start','name'])\n",
      "        l = list(df['name'])\n",
      "        string = makePattern(l)\n",
      "        num = len(lrs(string)[0])\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "def collect_stats(dictionary):\n",
      "    num = len(dictionary['start'])\n",
      "    if num == 0:\n",
      "        return None\n",
      "    avg_len  = np.mean(dictionary['durations'])\n",
      "    len_std  = np.std(dictionary['durations'])\n",
      "    if None not in dictionary['confidence']:\n",
      "        avg_conf = np.mean(dictionary['confidence'])\n",
      "        conf_std = np.std(dictionary['confidence'])\n",
      "    else:\n",
      "        avg_conf = -1\n",
      "        conf_std = -1\n",
      "    return (num, avg_len, len_std, avg_conf, conf_std)\n",
      "\n",
      "def printRow(tup):\n",
      "    if tup and \"nan\" not in tup:\n",
      "        print ','.join([repr(e) for e in tup])\n",
      "    \n",
      "#printRow(get_SALAMI_song_info(salami_ex))\n",
      "#gen_dataframe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# multiple distinct occurences\n",
      "def mdo(s, sub):\n",
      "    # if there are two or more distinct (non-overlapping) occurences of sub in s, return True\n",
      "    # else, return False\n",
      "    s = s.replace(sub, '', 1)\n",
      "    if s.replace(sub, '', 1) != s:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "\n",
      "# longest common prefix\n",
      "def lcp(s1, s2):\n",
      "    # return the longest common prefix b/t s1 and s2\n",
      "    N = min(len(s1), len(s2))\n",
      "    for i in range(N):\n",
      "        if s1[i] != s2[i]:\n",
      "            return s1[:i]\n",
      "    return s1\n",
      "    \n",
      "# longest repeated substrings\n",
      "# s is a non-delimited string of letters\n",
      "def lrs(s):\n",
      "    # make an array of all suffixes of s\n",
      "    N = len(s)\n",
      "    suffixes = []\n",
      "    for i in range(N):\n",
      "        suffixes.append(s[i:N])\n",
      "    print(suffixes)\n",
      "    \n",
      "    # sort the suffixes\n",
      "    suffixes.sort()\n",
      "    print(suffixes)\n",
      "    \n",
      "    # find longest repeated substrings by comparing adjacent sorted suffixes\n",
      "    LRS = ['']\n",
      "    for i in range(N-1):\n",
      "        x = lcp(suffixes[i], suffixes[i+1])\n",
      "        # only add subtrings to array if\n",
      "        # 1) there are two or more unique occurences of x in s\n",
      "        # 2) length of x >= length of elements in LRS\n",
      "        if mdo(s, x) and len(x) >= len(LRS[0]):\n",
      "            if len(x) > len(LRS[0]):\n",
      "                LRS = []\n",
      "            LRS.append(x)\n",
      "    return LRS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "import re\n",
      "\n",
      "##################\n",
      "# Decision Tree Utility Functions\n",
      "##################\n",
      "\n",
      "TARGET = 'genre'\n",
      "def learn_decision_tree(dataset):\n",
      "    if num_groups(dataset) == 1:\n",
      "        cls = re.split('[ \\t\\n\\r]+',repr(dataset['genre']))[1]\n",
      "        #print(\"  LEAF: '%s'\" % (cls))\n",
      "        return DTreeNode(cls)\n",
      "    #print(\"  INTERNAL %d %d\" % (num_groups(dataset),len(dataset)))\n",
      "    _,axis,threshold = find_optimal_split(dataset)\n",
      "    if axis == None or axis == '':\n",
      "        raise Exception\n",
      "    left_subset  = dataset[dataset[axis] <  threshold]\n",
      "    right_subset = dataset[dataset[axis] >= threshold]    \n",
      "    return DTreeNode(threshold,\n",
      "                     axis,\n",
      "                     learn_decision_tree(left_subset),\n",
      "                     learn_decision_tree(right_subset))\n",
      "\n",
      "\n",
      "def getRelevantFeatures(dataset):\n",
      "    # strip away columns not used for learning\n",
      "    return dataset.drop(['ID',\n",
      "                         'std_bar_len','avg_bar_conf','std_bar_conf',\n",
      "                         'std_beat_len', 'avg_beat_conf','std_beat_conf',\n",
      "                         'std_tatum_len', 'avg_tatum_conf','std_tatum_conf',\n",
      "                         'std_section_len', 'avg_section_conf','std_section_conf',\n",
      "                         'key_val','key_conf','tempo_conf'\n",
      "                       ], 1)\n",
      "\n",
      "def find_optimal_split(dataset):\n",
      "    (best_gain,best_axis,best_threshold) = (0,0,0)\n",
      "    axis_index = 0\n",
      "    features = dataset.columns.tolist()[:-1]\n",
      "    for axis in features:\n",
      "        uniq_data = dataset.sort(columns=axis,inplace=False)\n",
      "        uniq_data = uniq_data.drop_duplicates(subset=axis)\n",
      "        for index in range(0,len(uniq_data) - 1):\n",
      "            datum1 = uniq_data.iloc[index,axis_index]\n",
      "            datum2 = uniq_data.iloc[index + 1, axis_index]\n",
      "            threshold = datum1 + (abs(datum1 - datum2)/2)\n",
      "            gain = information_gain(dataset,axis,threshold)\n",
      "            if (gain > best_gain):\n",
      "                (best_gain,best_axis,best_threshold) = (gain,axis,threshold)\n",
      "        axis_index += 1\n",
      "    group_dict = dataset.groupby(TARGET).groups\n",
      "    return best_gain, best_axis, best_threshold\n",
      "        \n",
      "def information_gain(dataset, axis, threshold):\n",
      "    y = list(dataset[TARGET])\n",
      "    ye = entropy(y)\n",
      "    a = list(dataset[dataset[axis] <  threshold][TARGET])\n",
      "    ae = (1.0 * len(a)/len(y)) * entropy(a)\n",
      "    b = list(dataset[dataset[axis] >= threshold][TARGET])\n",
      "    be = (1.0 * len(b)/len(y)) * entropy(b)\n",
      "    return ye - ae - be\n",
      "    \n",
      "def entropy(array):\n",
      "    freq_dict = {}\n",
      "    for val in array:\n",
      "        if val not in list(freq_dict):\n",
      "            freq_dict[val]  = 1\n",
      "        else:\n",
      "            freq_dict[val] += 1\n",
      "    entropy = 0.0\n",
      "    total_count = 1.0 * len(array)\n",
      "    for val in list(freq_dict):\n",
      "        frequency = 1.0 * freq_dict[val] / total_count\n",
      "        entropy  -= frequency * math.log(frequency,2)\n",
      "    return entropy\n",
      "    \n",
      "def num_groups(dataset):\n",
      "    return len(dataset.groupby(TARGET).groups)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##################\n",
      "# Decision Tree Classes\n",
      "##################\n",
      "\n",
      "class DTreeNode:\n",
      "    def __init__(self,val,col=-1,l=None,r=None):\n",
      "        self.value = val\n",
      "        self.col = col\n",
      "        self.left = l\n",
      "        self.right = r\n",
      "\n",
      "    def decide (self, datarow):\n",
      "        if self.isLeaf():\n",
      "            return self.value\n",
      "        if (datarow[self.col] < self.value):\n",
      "            return self.left\n",
      "        return self.right\n",
      "\n",
      "    def isLeaf(self):\n",
      "        return self.left is None and self.right is None\n",
      "\n",
      "class DTree:\n",
      "    def __init__(self, filename):\n",
      "        dataframe = pd.read_csv(filename)\n",
      "        dataframe = getRelevantFeatures(dataframe)\n",
      "        self.root = learn_decision_tree(dataframe)\n",
      "        print 'Decision tree for %s has been learned' % filename\n",
      "\n",
      "    def decide (self, datarow):\n",
      "        return self.decide_rec(self.root, datarow)\n",
      "\n",
      "    def decide_rec(self, node, datarow):\n",
      "        dec = node.decide(datarow)\n",
      "        if type(dec) == type(\"\"):\n",
      "            return dec\n",
      "        return self.decide_rec(dec, datarow)\n",
      "    \n",
      " \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##################\n",
      "# Driver methods\n",
      "##################\n",
      "\n",
      "def rec_print(node, indent):\n",
      "    s = ' ' * indent\n",
      "    if (node.left != None):\n",
      "        print (s + 'if %s < %f:' % (node.col, node.value))\n",
      "        rec_print(node.left, indent + 2)\n",
      "        print (s + 'else:' % (node.col, node.value))\n",
      "        rec_print(node.right, indent + 2)\n",
      "    else:\n",
      "        print s + 'return ' + repr(node.value)\n",
      "        \n",
      "#rec_print(dTree.root, 0)\n",
      "\n",
      "def test_tree (train_fn,test_fn):\n",
      "    dTree = DTree(train_fn)\n",
      "    df = pd.read_csv(test_fn)\n",
      "    wrong = 0\n",
      "    total = len(df)\n",
      "    for _,row in df.iterrows():\n",
      "        ID = row['ID']\n",
      "        guess = dTree.decide(row)\n",
      "        truth = row['genre']\n",
      "        print ID, guess, truth\n",
      "        if guess != truth:\n",
      "            wrong += 1\n",
      "    print '%d incorrect out of %d (%.2f%%)' % (wrong,total,wrong * 100 / total)\n",
      "\n",
      "test_tree(SALAMI_path + 'our_data/train_first.csv', SALAMI_path + 'our_data/test_first.csv')\n",
      "print 'Done'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Decision tree for /home/matt/Development/cs580/project/data/SALAMI/our_data/train_first.csv has been learned\n",
        "TRFSJTS1447CA3CC97 R&B Classical\n",
        "TRAJFLU1447C9AFA2A World Rock\n",
        "TRZSFQZ1447C971B9B Rock Rock\n",
        "TRSXXSQ1447C44644B Classical Classical\n",
        "TRVFBZS1447CD37D09 World World\n",
        "TRPMFNV1447CD4D7B7 World World\n",
        "TRBVNPZ1447C711C79 Jazz Jazz\n",
        "TRZPCIL1447CB871D7 World World\n",
        "TRCZNIV1447C87E342 R&B R&B\n",
        "TRAQIGA1447C397351 Rock Rock\n",
        "TRFFPFY1447CB70C8C World World\n",
        "TRHTVWB1447C05AD5E Rock Rock\n",
        "TRIMHTQ1447C7A150E Jazz Jazz\n",
        "TRWYEHU1447C3617FC Classical Classical\n",
        "TRONIBC1447C39F2EE Classical Classical\n",
        "TRCUAGZ1447C66E794 Blues Jazz\n",
        "TRKHNGF1447C3AF859 Classical Classical\n",
        "TREWRFJ1447CC247C0 Classical World\n",
        "TROJUOT1447C8690FF R&B R&B\n",
        "TRXMWVZ1447C7DA335 Jazz Jazz\n",
        "TRZXARZ1447C741CCA Jazz Jazz\n",
        "TRBNEDA1456125D3D2 Classical Classical\n",
        "TRDQZMG14561265594 Classical Classical\n",
        "TRKZUFA1447C7962FC Jazz Jazz\n",
        "TRDBNZN1447CC11D7B World World\n",
        "TRCKLYA1447CB2B44D World World\n",
        "TRUSYEQ1447C4B7205 Classical Classical\n",
        "TRBMMEL1447C80EBE0 Jazz Jazz\n",
        "TRRPGEK1447C72C520 Jazz Jazz\n",
        "TRJKSVA1447CAE80F8 World World\n",
        "TRSYAMD1456124CC33 Classical Classical\n",
        "TRFJTND1447C66323C Jazz Jazz\n",
        "TRQMYLC1447C67C442 Jazz Jazz\n",
        "TRRLSUP1447CB4FD1A World World\n",
        "TRKBJUS1447CD04039 World World\n",
        "TRCPBQI1447C3BCD6F World Classical\n",
        "TRXRAEX1447CC9AFF1 World World\n",
        "TRDGHES1447C8A18B9 Rock R&B\n",
        "TRAHZAF1447C42F234 Classical Classical\n",
        "6 incorrect out of 39 (15.00%)\n",
        "Done\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}