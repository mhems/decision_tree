{
 "metadata": {
  "name": "",
  "signature": "sha256:2ae7795b56013a3feb91f46b4ea67ac3c2ca276e775ab523a29451ede46696a0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import tables\n",
      "import re\n",
      "import glob\n",
      "import os\n",
      "from StringIO import StringIO\n",
      "\n",
      "############\n",
      "## Aggregates data from all json files into cvs summary\n",
      "############\n",
      "\n",
      "data_path = \"/home/matt/Development/cs580/project/data/\"\n",
      "SALAMI_path = data_path + \"SALAMI/\" # json\n",
      "\n",
      "salami_ex = SALAMI_path + 'echonest_features/1060/echonest_data.json'\n",
      "\n",
      "# series -> index | split | records\n",
      "# frame  -> columns | split | records | index | values\n",
      "\n",
      "def gen_dataframe():\n",
      "    print \"ID, num_bars,avg_bar_len,std_bar_len,avg_bar_conf,std_bar_conf,\" \\\n",
      "              \"num_beats,avg_beat_len,std_beat_len,avg_beat_conf,std_beat_conf,\" \\\n",
      "              \"num_tatums,avg_tatum_len,std_tatum_len,avg_tatum_conf,std_tatum_conf,\" \\\n",
      "              \"num_sections,avg_section_len,std_section_len,avg_section_conf,std_section_conf,\"\\\n",
      "              \"duration,key_val,key_conf,tempo_val,tempo_conf,\"\\\n",
      "              \"len_longest_pattern_upper,num_uniq_groups_upper,avg_len_upper,\"\\\n",
      "              \"len_longest_pattern_lower,num_uniq_groups_lower,avg_len_lower,\"\\\n",
      "              \"genre\"\n",
      "    for songpath in glob.glob(SALAMI_path + 'echonest_features/*/*.json'):\n",
      "        printRow(get_SALAMI_song_info(songpath))\n",
      "\n",
      "def get_SALAMI_song_info(songpath):\n",
      "    df = pd.read_json(songpath, typ='series')\n",
      "    metadata = df['metadata']\n",
      "    ID = metadata['identifier']\n",
      "    bars_tuple     = collect_stats(df['bars'])\n",
      "    beats_tuple    = collect_stats(df['beats'])\n",
      "    tatums_tuple   = collect_stats(df['tatums'])\n",
      "    sections_tuple = collect_stats(df['sections'])\n",
      "    if bars_tuple is None or beats_tuple is None or tatums_tuple is None or sections_tuple is None:\n",
      "        return None\n",
      "    duration = metadata['duration']\n",
      "    key_val = metadata['key']['value']\n",
      "    key_conf = metadata['key']['confidence']\n",
      "    tempo_val = metadata['tempo']['value']\n",
      "    tempo_conf = metadata['tempo']['confidence']\n",
      "    if 'genre' in metadata.keys() and metadata['genre'] != \"\":\n",
      "        genre = metadata['genre']\n",
      "    else:\n",
      "        return None\n",
      "    clustering = getClusterData(songpath)\n",
      "    if clustering is None:\n",
      "        return None\n",
      "    ret = [ID]\n",
      "    ret.extend(bars_tuple)\n",
      "    ret.extend(beats_tuple)\n",
      "    ret.extend(tatums_tuple)\n",
      "    ret.extend(sections_tuple)\n",
      "    ret.append(duration)\n",
      "    ret.append(key_val)\n",
      "    ret.append(key_conf)\n",
      "    ret.append(tempo_val)\n",
      "    ret.append(tempo_conf)\n",
      "    ret.extend(clustering)\n",
      "    ret.append(genre)\n",
      "    return tuple(ret)\n",
      "\n",
      "def makePattern(ls):\n",
      "    appeared = {}\n",
      "    l = []\n",
      "    chrs = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
      "            'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
      "    counter = 0\n",
      "    for e in ls:\n",
      "        if e != 'Silence' and e != 'End':\n",
      "            l.append(e)\n",
      "            if e not in appeared.keys():\n",
      "                appeared[e] = chrs[counter]\n",
      "                counter += 1\n",
      "    if counter > len(chrs):\n",
      "        raise Exception(repr(counter))\n",
      "    return ''.join([appeared[k] for k in l]), len(appeared.keys())\n",
      "       \n",
      "def parse_file(filepath):\n",
      "    df = pd.read_table(filepath, delim_whitespace=True, names=['start','name'])\n",
      "    l = list(df['name'])\n",
      "    pattern, sz = makePattern(l)\n",
      "    num = len(lrs(pattern)[0])\n",
      "    array = list(df['start'])\n",
      "    avg = array[0]\n",
      "    for i in range(0,len(array)-1):\n",
      "        avg += array[i+1] - array[i]\n",
      "    avg /= len(array)\n",
      "    return (num,sz,avg)\n",
      "\n",
      "# return (num upper, len upper, avg upper, num lower, len lower, avg lower)\n",
      "def getClusterData(songpath):\n",
      "    file_dir = re.search('features/([0-9]+)',songpath).group(1)\n",
      "    filepath = SALAMI_path + 'data/' + file_dir + '/parsed/'\n",
      "    ret = None\n",
      "    if os.path.exists(filepath):\n",
      "        ret = []\n",
      "        lowercase_f = filepath + 'textfile1_lowercase.txt'\n",
      "        uppercase_f = filepath + 'textfile1_uppercase.txt'\n",
      "        if (not os.path.isfile(lowercase_f)) or (not os.path.isfile(uppercase_f)):\n",
      "            lowercase_f = filepath + 'textfile2_lowercase.txt'\n",
      "            uppercase_f = filepath + 'textfile2_uppercase.txt'\n",
      "            if (not os.path.isfile(lowercase_f)) or (not os.path.isfile(uppercase_f)):\n",
      "                return None\n",
      "        ret.extend(parse_file(lowercase_f))\n",
      "        ret.extend(parse_file(uppercase_f))\n",
      "    return ret\n",
      "\n",
      "def collect_stats(dictionary):\n",
      "    num = len(dictionary['start'])\n",
      "    if num == 0:\n",
      "        return None\n",
      "    avg_len  = np.mean(dictionary['durations'])\n",
      "    len_std  = np.std(dictionary['durations'])\n",
      "    if None not in dictionary['confidence']:\n",
      "        avg_conf = np.mean(dictionary['confidence'])\n",
      "        conf_std = np.std(dictionary['confidence'])\n",
      "    else:\n",
      "        avg_conf = -1\n",
      "        conf_std = -1\n",
      "    return (num, avg_len, len_std, avg_conf, conf_std)\n",
      "\n",
      "def printRow(tup):\n",
      "    if tup and \"nan\" not in tup:\n",
      "        print ','.join([repr(e) for e in tup])\n",
      "    \n",
      "#printRow(get_SALAMI_song_info(salami_ex))\n",
      "#gen_dataframe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# multiple distinct occurences\n",
      "def mdo(s, sub):\n",
      "    # if there are two or more distinct (non-overlapping) occurences of sub in s, return True\n",
      "    # else, return False\n",
      "    s = s.replace(sub, '', 1)\n",
      "    if s.replace(sub, '', 1) != s:\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "\n",
      "# longest common prefix\n",
      "def lcp(s1, s2):\n",
      "    # return the longest common prefix b/t s1 and s2\n",
      "    N = min(len(s1), len(s2))\n",
      "    for i in range(N):\n",
      "        if s1[i] != s2[i]:\n",
      "            return s1[:i]\n",
      "    return s1\n",
      "    \n",
      "# longest repeated substrings\n",
      "# s is a non-delimited string of letters\n",
      "def lrs(s):\n",
      "    # make an array of all suffixes of s\n",
      "    N = len(s)\n",
      "    suffixes = []\n",
      "    for i in range(N):\n",
      "        suffixes.append(s[i:N])\n",
      "    #print(suffixes)\n",
      "    \n",
      "    # sort the suffixes\n",
      "    suffixes.sort()\n",
      "    #print(suffixes)\n",
      "    \n",
      "    # find longest repeated substrings by comparing adjacent sorted suffixes\n",
      "    LRS = ['']\n",
      "    for i in range(N-1):\n",
      "        x = lcp(suffixes[i], suffixes[i+1])\n",
      "        # only add subtrings to array if\n",
      "        # 1) there are two or more unique occurences of x in s\n",
      "        # 2) length of x >= length of elements in LRS\n",
      "        if mdo(s, x) and len(x) >= len(LRS[0]):\n",
      "            if len(x) > len(LRS[0]):\n",
      "                LRS = []\n",
      "            LRS.append(x)\n",
      "    return LRS"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import math\n",
      "import re\n",
      "import glob\n",
      "import math\n",
      "\n",
      "TARGET = 'genre'\n",
      "\n",
      "def entropy(array):\n",
      "    freq_dict = {}\n",
      "    for val in array:\n",
      "        if val not in list(freq_dict):\n",
      "            freq_dict[val]  = 1\n",
      "        else:\n",
      "            freq_dict[val] += 1\n",
      "    entropy = 0.0\n",
      "    total_count = 1.0 * len(array)\n",
      "    for val in list(freq_dict):\n",
      "        frequency = 1.0 * freq_dict[val] / total_count\n",
      "        entropy  -= frequency * math.log(frequency,2)\n",
      "    return entropy\n",
      "\n",
      "def information_gain(dataset, axis, threshold):\n",
      "    y = list(dataset[TARGET])\n",
      "    ye = entropy(y)\n",
      "    a = list(dataset[dataset[axis] <  threshold][TARGET])\n",
      "    ae = (1.0 * len(a)/len(y)) * entropy(a)\n",
      "    b = list(dataset[dataset[axis] >= threshold][TARGET])\n",
      "    be = (1.0 * len(b)/len(y)) * entropy(b)\n",
      "    return ye - ae - be\n",
      "\n",
      "def find_optimal_split(dataset):\n",
      "    (best_gain,best_axis,best_threshold) = (0,0,0)\n",
      "    axis_index = 0\n",
      "    features = dataset.columns.tolist()[:-1]\n",
      "    for axis in features:\n",
      "        uniq_data = dataset.sort(columns=axis,inplace=False)\n",
      "        uniq_data = uniq_data.drop_duplicates(subset=axis)\n",
      "        for index in range(0,len(uniq_data) - 1):\n",
      "            datum1 = uniq_data.iloc[index,axis_index]\n",
      "            datum2 = uniq_data.iloc[index + 1, axis_index]\n",
      "            threshold = datum1 + (abs(datum1 - datum2)/2)\n",
      "            gain = information_gain(dataset,axis,threshold)\n",
      "            if (gain > best_gain):\n",
      "                (best_gain,best_axis,best_threshold) = (gain,axis,threshold)\n",
      "        axis_index += 1\n",
      "    group_dict = dataset.groupby(TARGET).groups\n",
      "    return best_gain, best_axis, best_threshold\n",
      "    \n",
      "class DTreeNode:\n",
      "    def __init__(self,val,col=-1,l=None,r=None):\n",
      "        self.value = val\n",
      "        self.col = col\n",
      "        self.left = l\n",
      "        self.right = r\n",
      "\n",
      "    def decide (self, datarow):\n",
      "        if self.isLeaf():\n",
      "            return self.value\n",
      "        if (datarow[self.col] < self.value):\n",
      "            return self.left\n",
      "        return self.right\n",
      "\n",
      "    def isLeaf(self):\n",
      "        return self.left is None and self.right is None\n",
      "\n",
      "def num_groups(dataset):\n",
      "    return len(dataset.groupby(TARGET).groups)\n",
      "\n",
      "def learn_decision_tree(dataset):\n",
      "    if num_groups(dataset) == 1:\n",
      "        cls = re.split('[ \\t\\n\\r]+',repr(dataset['genre']))[1]\n",
      "        #print(\"  LEAF: '%s'\" % (cls))\n",
      "        return DTreeNode(cls)\n",
      "    #print(\"  INTERNAL %d %d\" % (num_groups(dataset),len(dataset)))\n",
      "    _,axis,threshold = find_optimal_split(dataset)\n",
      "    if axis == None or axis == '':\n",
      "        raise Exception\n",
      "    left_subset  = dataset[dataset[axis] <  threshold]\n",
      "    right_subset = dataset[dataset[axis] >= threshold]    \n",
      "    return DTreeNode(threshold,\n",
      "                     axis,\n",
      "                     learn_decision_tree(left_subset),\n",
      "                     learn_decision_tree(right_subset))\n",
      "\n",
      "\n",
      "def getRelevantFeatures(dataset):\n",
      "    # strip away columns not used for learning\n",
      "    return dataset.drop(['ID',\n",
      "                         'std_bar_len','avg_bar_conf','std_bar_conf',\n",
      "                         'std_beat_len', 'avg_beat_conf','std_beat_conf',\n",
      "                         'std_tatum_len', 'avg_tatum_conf','std_tatum_conf',\n",
      "                         'std_section_len', 'avg_section_conf','std_section_conf',\n",
      "                         'key_val','key_conf','tempo_conf'\n",
      "                       ], 1)\n",
      "\n",
      "class DTree:\n",
      "    def __init__(self, filename):\n",
      "        dataframe = pd.read_csv(filename)\n",
      "        dataframe = getRelevantFeatures(dataframe)\n",
      "        print 'Learning decision tree'\n",
      "        self.root = learn_decision_tree(dataframe)\n",
      "        print 'Decision tree for %s has been learned' % filename\n",
      "\n",
      "    def decide (self, datarow):\n",
      "        return self.decide_rec(self.root, datarow)\n",
      "\n",
      "    def decide_rec(self, node, datarow):\n",
      "        dec = node.decide(datarow)\n",
      "        if type(dec) == type(\"\"):\n",
      "            return dec\n",
      "        return self.decide_rec(dec, datarow)\n",
      "\n",
      "def test_tree (train_fn,test_fn):\n",
      "    dTree = DTree(train_fn)\n",
      "    df    = pd.read_csv(test_fn)\n",
      "    wrong = 0\n",
      "    total = len(df)\n",
      "    for _,row in df.iterrows():\n",
      "        ID = row['ID']\n",
      "        guess = dTree.decide(row)\n",
      "        truth = row['genre']\n",
      "        print ID, guess, truth\n",
      "        if guess != truth:\n",
      "            wrong += 1\n",
      "    print 'File %s::%d incorrect out of %d (%.2f%%)' % (train_fn,wrong,total,wrong * 100 / total)\n",
      "\n",
      "def rec_print(node, indent):\n",
      "    s = ' ' * indent\n",
      "    if (node.left != None):\n",
      "        print (s + 'if %s < %f:' % (node.col, node.value))\n",
      "        rec_print(node.left, indent + 2)\n",
      "        print (s + 'else:' % (node.col, node.value))\n",
      "        rec_print(node.right, indent + 2)\n",
      "    else:\n",
      "        print s + 'return ' + repr(node.value)\n",
      "        \n",
      "SALAMI_path = '/home/matt/Development/cs580/project/repo/salami_data/runs/'\n",
      "for i in range(1,11):\n",
      "    test_tree(SALAMI_path + 'train_' + repr(i) + '.csv',\n",
      "             SALAMI_path + 'test_'  + repr(i) + '.csv')\n",
      "    print '*' * 10\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}